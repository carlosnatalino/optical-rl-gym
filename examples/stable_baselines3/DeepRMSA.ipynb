{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical RL-Gym\n",
    "\n",
    "## Training the Stable Baselines agents using the DeepRMSA environment\n",
    "\n",
    "This file contains examples of how to train agents for the DeepRMSA environment.\n",
    "Note that the DeepRMSA work proposes both an environment and an agent, and this tool only implements the environment part.\n",
    "\n",
    "The agents used in this file come from the [Stable-baselines3](https://github.com/DLR-RM/stable-baselines3) framework.\n",
    "\n",
    "Before running this notebook, make sure to install Stable-Baselines3 and the Optical RL-Gym in your Python environment (see the requirements.txt file in this folder).\n",
    "\n",
    "### Attention\n",
    "\n",
    "Due to changes introduced in [Stable-Baselines3 version 2](https://github.com/DLR-RM/stable-baselines3/releases/tag/v2.0.0) the compatibility with OpenAI Gym was broken.\n",
    "This tool remains using OpenAI Gym, so the version of Stable-Baselines3 has been locked to `<2.0.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Baseline imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "stable_baselines3.__version__ # printing out stable_baselines version used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment imports\n",
    "\n",
    "In this particular example, there is no need to import anything specific to the Optical RL-Gym. Only by importing the Open AI Gym below, you already get all the functionality needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback from https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training\n",
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1, show_plot: bool=False):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.show_plot = show_plot\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        if self.show_plot and self.n_calls % self.check_freq == 0 and self.n_calls > 5001:\n",
    "            plotting_average_window = 100\n",
    "\n",
    "            training_data = pd.read_csv(self.log_dir + 'training.monitor.csv', skiprows=1)\n",
    "\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9.6, 4.8))\n",
    "\n",
    "            ax1.plot(np.convolve(training_data['r'], np.ones(plotting_average_window)/plotting_average_window, mode='valid'))\n",
    "\n",
    "            ax1.set_xlabel('Episode')\n",
    "            ax1.set_ylabel('Reward')\n",
    "\n",
    "            ax2.semilogy(np.convolve(training_data['episode_service_blocking_rate'], np.ones(plotting_average_window)/plotting_average_window, mode='valid'))\n",
    "\n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Episode service blocking rate')\n",
    "\n",
    "            ax3.semilogy(np.convolve(training_data['episode_bit_rate_blocking_rate'], np.ones(plotting_average_window)/plotting_average_window, mode='valid'))\n",
    "\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Episode bit rate blocking rate')\n",
    "\n",
    "            # fig.get_size_inches()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                 # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Num timesteps: {} - \".format(self.num_timesteps), end=\"\")\n",
    "                    print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "                  # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                        self.model.save(self.save_path)\n",
    "                if self.verbose > 0:\n",
    "                    clear_output(wait=True)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "The parameters are set as in the [DeepRMSA](https://doi.org/10.1109/JLT.2019.2923615) work and its [available reporitory](https://github.com/xiaoliangchenUCD/DeepRMSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the topology binary file containing the graph and the k-shortest paths\n",
    "# if you want to generate your own binary topology file, check examples/create_topology_rmsa.py\n",
    "topology_name = 'nsfnet_chen'\n",
    "k_paths = 5\n",
    "with open(f'../topologies/{topology_name}_{k_paths}-paths_6-modulations.h5', 'rb') as f:\n",
    "    topology = pickle.load(f)\n",
    "\n",
    "monitor_info_keywords=('episode_service_blocking_rate','episode_bit_rate_blocking_rate')\n",
    "\n",
    "# node probabilities from https://github.com/xiaoliangchenUCD/DeepRMSA/blob/6708e9a023df1ec05bfdc77804b6829e33cacfe4/Deep_RMSA_A3C.py#L77\n",
    "node_request_probabilities = np.array([0.01801802, 0.04004004, 0.05305305, 0.01901902, 0.04504505,\n",
    "       0.02402402, 0.06706707, 0.08908909, 0.13813814, 0.12212212,\n",
    "       0.07607608, 0.12012012, 0.01901902, 0.16916917])\n",
    "\n",
    "# mean_service_holding_time=7.5,\n",
    "env_args = dict(topology=topology, seed=10, \n",
    "                allow_rejection=False, # the agent cannot proactively reject a request\n",
    "                j=1, # consider only the first suitable spectrum block for the spectrum assignment\n",
    "                mean_service_holding_time=7.5, # value is not set as in the paper to achieve comparable reward values\n",
    "                episode_length=50, node_request_probabilities=node_request_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topology.edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the monitors and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = \"./tmp/deeprmsa-ppo/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir, show_plot=False)\n",
    "\n",
    "env = gym.make('DeepRMSA-v0', **env_args)\n",
    "\n",
    "# logs will be saved in log_dir/training.monitor.csv\n",
    "# in this case, on top of the usual monitored things, we also monitor service and bit rate blocking rates\n",
    "env = Monitor(env, log_dir + 'training', info_keywords=monitor_info_keywords)\n",
    "# for more information about the monitor, check https://stable-baselines.readthedocs.io/en/master/_modules/stable_baselines/bench/monitor.html#Monitor\n",
    "\n",
    "# here goes the arguments of the policy network to be used\n",
    "policy_args = dict(net_arch=5*[128]) # we use the elu activation function\n",
    "\n",
    "agent = PPO(MlpPolicy, env, verbose=0, tensorboard_log=\"./tb/PPO-DeepRMSA-v0/\", policy_kwargs=policy_args, gamma=.95, learning_rate=10e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "a = agent.learn(total_timesteps=10_000_000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results_plotter.plot_results([log_dir], 1e5, results_plotter.X_TIMESTEPS, \"DeepRMSA PPO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to load the monitor data\n",
    "training_data = pd.read_csv(log_dir + 'training.monitor.csv', skiprows=1)\n",
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_average_window = 100\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(9.6, 4.8))\n",
    "\n",
    "ax1.plot(np.convolve(training_data['r'], np.ones(plotting_average_window)/plotting_average_window, mode='valid'))\n",
    "\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Reward')\n",
    "\n",
    "ax2.semilogy(np.convolve(training_data['episode_service_blocking_rate'], np.ones(plotting_average_window)/plotting_average_window, mode='valid'))\n",
    "\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Episode service blocking rate')\n",
    "\n",
    "ax3.semilogy(np.convolve(training_data['episode_bit_rate_blocking_rate'], np.ones(plotting_average_window)/plotting_average_window, mode='valid'))\n",
    "\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Episode bit rate blocking rate')\n",
    "\n",
    "# fig.get_size_inches()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "61de2f18b207227d3c4fc84cfc17e507d57d4765ad8aad4f4d26ef6793923bed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
